{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6af05e-79e1-45fb-86df-6a3909ddefb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Count</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Features</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536365</th>\n",
       "      <td>[85123A, 71053, 84406B, 84029G, 84029E, 22752,...</td>\n",
       "      <td>7</td>\n",
       "      <td>[3, 115, 116, 117, 118, 119, 120]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536366</th>\n",
       "      <td>[22633, 22632]</td>\n",
       "      <td>2</td>\n",
       "      <td>[121, 122]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536367</th>\n",
       "      <td>[84879, 22745, 22748, 22749, 22310, 84969, 226...</td>\n",
       "      <td>12</td>\n",
       "      <td>[123, 124, 125, 126, 127, 128, 129, 130, 131, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536368</th>\n",
       "      <td>[22960, 22913, 22912, 22914]</td>\n",
       "      <td>4</td>\n",
       "      <td>[135, 136, 137, 138]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536372</th>\n",
       "      <td>[22632, 22633]</td>\n",
       "      <td>2</td>\n",
       "      <td>[122, 121]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   StockCode  Count  \\\n",
       "InvoiceNo                                                             \n",
       "536365     [85123A, 71053, 84406B, 84029G, 84029E, 22752,...      7   \n",
       "536366                                        [22633, 22632]      2   \n",
       "536367     [84879, 22745, 22748, 22749, 22310, 84969, 226...     12   \n",
       "536368                          [22960, 22913, 22912, 22914]      4   \n",
       "536372                                        [22632, 22633]      2   \n",
       "\n",
       "                                                      Tokens  \\\n",
       "InvoiceNo                                                      \n",
       "536365                     [3, 115, 116, 117, 118, 119, 120]   \n",
       "536366                                            [121, 122]   \n",
       "536367     [123, 124, 125, 126, 127, 128, 129, 130, 131, ...   \n",
       "536368                                  [135, 136, 137, 138]   \n",
       "536372                                            [122, 121]   \n",
       "\n",
       "                                                    Features  \\\n",
       "InvoiceNo                                                      \n",
       "536365     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "536366     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "536367     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "536368     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "536372     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                      Target  \n",
       "InvoiceNo                                                     \n",
       "536365     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "536366     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "536367     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "536368     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "536372     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "df = pd.read_csv('Raw Database.csv')\n",
    "df['Description'] = df['Description'].str.strip()\n",
    "df = df[~df['InvoiceNo'].str.contains('C')]\n",
    "df = df[~df['StockCode'].str.contains('POST')]\n",
    "df2 = (df[df['Country'] == 'United Kingdom'])\n",
    "df2 = df2[['InvoiceNo', 'StockCode']].groupby('InvoiceNo').agg({'StockCode': lambda x: (list(x))})\n",
    "df2['Count'] = df2['StockCode'].apply(lambda x: len(x))\n",
    "df2.drop(df2[df2.Count < 2].index, inplace=True)\n",
    "df2.drop(df2[df2.Count > 30].index, inplace=True)\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(df.StockCode.unique())\n",
    "df2['Tokens'] = df2.StockCode.apply(lambda x: list(filter(None, tk.texts_to_sequences(x))))\n",
    "df2['Tokens'] = df2['Tokens'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "df2['Features'] = df2.Tokens.apply(lambda x: x[:-1])\n",
    "df2['Features'] = df2.Features.apply(lambda x: ((30-len(x)) * [0] + x)[:])\n",
    "df2['Target'] = df2.Tokens.apply(lambda x: x[1:])\n",
    "df2['Target'] = df2.Target.apply(lambda x: ((30-len(x)) * [0] + x)[:])\n",
    "df2.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82fe9815-9f34-4bfd-8c92-e57e38d5d425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=({'item_id': TensorSpec(shape=(None, 30), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 30), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "def create_train_tfdata(train_feat_dict, train_target_tensor,\n",
    "                        batch_size, buffer_size=None):\n",
    "    \"\"\"\n",
    "    Create train tf dataset for model train input\n",
    "    :param train_feat_dict: dict, containing the features tensors for train data\n",
    "    :param train_target_tensor: np.array(), the training TARGET tensor\n",
    "    :param batch_size: (int) size of the batch to work with\n",
    "    :param buffer_size: (int) Optional. Default is None. Size of the buffer\n",
    "    :return: (tuple) 1st element is the training dataset,\n",
    "                     2nd is the number of steps per epoch (based on batch size)\n",
    "    \"\"\"\n",
    "    if buffer_size is None:\n",
    "        buffer_size = batch_size*50\n",
    "\n",
    "    train_steps_per_epoch = len(train_target_tensor) // batch_size\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_feat_dict,\n",
    "                                                        train_target_tensor)).cache()\n",
    "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    train_dataset = train_dataset.repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, train_steps_per_epoch\n",
    "\n",
    "\n",
    "train_feat_dict = {'item_id': df2['Features'].to_list()}\n",
    "train_target_tensor = df2['Target'].to_list()\n",
    "\n",
    "train_dataset, train_steps_per_epoch = create_train_tfdata(train_feat_dict,\n",
    "                                                         train_target_tensor,\n",
    "                                                         batch_size=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccbe7b1e-afbe-477f-9631-97333ed80418",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     66\u001b[0m hp \u001b[38;5;241m=\u001b[39m {train_dataset}\n\u001b[0;32m---> 67\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStockCode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 23\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(hp, max_len, item_vocab_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m     encoding_padding_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlogical_not(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mequal(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# # nb_days bucketized\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#                                    name='nb_days', dtype=tf.int32)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Remember that vocab_size is len of item tokenizer + 1\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# (for the padding '0' value)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     embedding_item \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(input_dim\u001b[38;5;241m=\u001b[39mitem_vocab_size,\n\u001b[0;32m---> 23\u001b[0m                                                output_dim\u001b[38;5;241m=\u001b[39m\u001b[43mhp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_item\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     24\u001b[0m                                                name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_item\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m                                               )(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     # nbins=100, +1 for zero padding\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#     embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#                                                   output_dim=hp.get('embedding_nb_days'),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     concat_embedding_input = tf.keras.layers.Concatenate(\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#      name='concat_embedding_input')([embedding_item, embedding_nb_days])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     concat_embedding_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBatchNormalization(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchnorm_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)(embedding_item)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "def build_model(hp, max_len, item_vocab_size):\n",
    "    \"\"\"\n",
    "    Build a model given the hyper-parameters with item and nb_days input features\n",
    "    :param hp: (kt.HyperParameters) hyper-parameters to use when building this model\n",
    "    :return: built and compiled tensorflow model \n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    inputs['item_id'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "                                       name='item_id', dtype=tf.int32)\n",
    "    # create encoding padding mask\n",
    "    encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['item_id'], 0))\n",
    "\n",
    "    # # nb_days bucketized\n",
    "    # inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
    "    #                                    name='nb_days', dtype=tf.int32)\n",
    "\n",
    "    # Pass categorical input through embedding layer\n",
    "    # with size equals to tokenizer vocabulary size\n",
    "    # Remember that vocab_size is len of item tokenizer + 1\n",
    "    # (for the padding '0' value)\n",
    "    \n",
    "    embedding_item = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
    "                                               output_dim=hp.get('embedding_item'),\n",
    "                                               name='embedding_item'\n",
    "                                              )(inputs['item_id'])\n",
    "#     # nbins=100, +1 for zero padding\n",
    "#     embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\n",
    "#                                                   output_dim=hp.get('embedding_nb_days'),\n",
    "#                                                   name='embedding_nb_days'\n",
    "#                                                  )(inputs['nb_days'])\n",
    "\n",
    "#     #  Concatenate embedding layers\n",
    "#     concat_embedding_input = tf.keras.layers.Concatenate(\n",
    "#      name='concat_embedding_input')([embedding_item, embedding_nb_days])\n",
    "\n",
    "    concat_embedding_input = tf.keras.layers.BatchNormalization(name='batchnorm_inputs')(embedding_item)\n",
    "    \n",
    "    # LSTM layer\n",
    "    rnn = tf.keras.layers.LSTM(units=hp.get('rnn_units_cat'),\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=False,\n",
    "                                   recurrent_initializer='glorot_normal',\n",
    "                                   name='LSTM_cat'\n",
    "                                   )(concat_embedding_input)\n",
    "\n",
    "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm')(rnn)\n",
    "\n",
    "    # Self attention so key=value in inputs\n",
    "    att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                    name='attention')(inputs=[rnn, rnn],\n",
    "                                                      mask=[encoding_padding_mask,\n",
    "                                                            encoding_padding_mask])\n",
    "\n",
    "    # Last layer is a fully connected one\n",
    "    output = tf.keras.layers.Dense(item_vocab_size, name='output')(att)\n",
    "\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.get('learning_rate')),\n",
    "        loss=loss_function,\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "hp = {train_dataset}\n",
    "model = build_model(hp, 30, (df.StockCode.unique().size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96a6d4-8951-451a-8791-6167bfe4e1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
